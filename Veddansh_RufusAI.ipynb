{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask, request, jsonify\n",
        "import threading\n",
        "from bs4 import BeautifulSoup\n",
        "import openai\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "\n",
        "# Setup driver with Chrome in headless mode\n",
        "def setup_driver():\n",
        "    chrome_options = Options()\n",
        "    chrome_options.add_argument(\"--headless\")\n",
        "    chrome_options.add_argument(\"--disable-gpu\")\n",
        "    chrome_options.add_argument(\"--no-sandbox\")\n",
        "\n",
        "    driver = webdriver.Chrome(options=chrome_options)\n",
        "    return driver\n",
        "\n",
        "# Initialize Flask app\n",
        "app = Flask(__name__)\n",
        "\n",
        "# Route to handle scraping and filtering\n",
        "@app.route('/scrape', methods=['POST'])\n",
        "def scrape():\n",
        "    data = request.json\n",
        "    url = data.get('url')\n",
        "    prompt = data.get('prompt')\n",
        "\n",
        "    if not url or not prompt:\n",
        "        return jsonify({\"error\": \"Please provide both a URL and a prompt.\"}), 400\n",
        "\n",
        "    # Scraping function: Crawl page and extract HTML content\n",
        "    def crawl_page(url):\n",
        "        driver = setup_driver()\n",
        "        driver.get(url)\n",
        "        page_source = driver.page_source\n",
        "        driver.quit()\n",
        "        return page_source\n",
        "\n",
        "    # Crawl the webpage\n",
        "    page_source = crawl_page(url)\n",
        "\n",
        "    # Optimized extraction: Get first few paragraphs to minimize processing\n",
        "    def extract_content(page_source):\n",
        "        soup = BeautifulSoup(page_source, 'html.parser')\n",
        "\n",
        "        # Limit extraction to first 5 <p> tags for speed\n",
        "        paragraphs = soup.find_all('p', limit=5)\n",
        "        paragraph_texts = [p.get_text() for p in paragraphs]\n",
        "\n",
        "        return paragraph_texts\n",
        "\n",
        "    # Extract relevant content from page\n",
        "    extracted_content = extract_content(page_source)\n",
        "\n",
        "    # Initialize OpenAI API\n",
        "    openai.api_key = '*your_openai_key*'\n",
        "\n",
        "    # Function to determine relevance of content using OpenAI\n",
        "    def is_content_relevant(prompt, content):\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are an assistant that helps decide if content is relevant to a user's prompt.\"},\n",
        "                {\"role\": \"user\", \"content\": f\"Based on the instruction '{prompt}', is the following content relevant: {content}?\"}\n",
        "            ]\n",
        "        )\n",
        "        answer = response['choices'][0]['message']['content'].strip().lower()\n",
        "        return 'yes' in answer\n",
        "\n",
        "    # Filter content in larger chunks instead of individual paragraphs for speed\n",
        "    def filter_relevant_content(prompt, content_list):\n",
        "        chunked_content = \" \".join(content_list[:2])  # Create a chunk of the first two paragraphs\n",
        "        if is_content_relevant(prompt, chunked_content):\n",
        "            return chunked_content\n",
        "        return \"No relevant content found.\"\n",
        "\n",
        "    # Filter the content based on prompt\n",
        "    relevant_content = filter_relevant_content(prompt, extracted_content)\n",
        "\n",
        "    # Return JSON response\n",
        "    return jsonify({\n",
        "        \"url\": url,\n",
        "        \"prompt\": prompt,\n",
        "        \"relevant_content\": relevant_content\n",
        "    })\n",
        "\n",
        "# Run Flask in background thread\n",
        "def run_flask():\n",
        "    app.run(port=5000, debug=True, use_reloader=False)\n",
        "\n",
        "threading.Thread(target=run_flask).start()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1mOgS5Ap9YT",
        "outputId": "5cdd3993-857a-4661-cb32-2fb4c21bfce4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Serving Flask app '__main__'\n",
            " * Debug mode: on\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask, request, jsonify\n",
        "import threading\n",
        "from bs4 import BeautifulSoup\n",
        "import openai\n",
        "import requests  # Using requests for faster page fetching\n",
        "\n",
        "# Initialize Flask app\n",
        "app = Flask(__name__)\n",
        "\n",
        "# Route to handle scraping and filtering\n",
        "@app.route('/scrape', methods=['POST'])\n",
        "def scrape():\n",
        "    data = request.json\n",
        "    url = data.get('url')\n",
        "    prompt = data.get('prompt')\n",
        "\n",
        "    if not url or not prompt:\n",
        "        return jsonify({\"error\": \"Please provide both a URL and a prompt.\"}), 400\n",
        "\n",
        "    # Use requests to fetch the webpage\n",
        "    def crawl_page(url):\n",
        "        response = requests.get(url)\n",
        "        if response.status_code == 200:\n",
        "            return response.text\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    # Crawl the webpage\n",
        "    page_source = crawl_page(url)\n",
        "    if page_source is None:\n",
        "        return jsonify({\"error\": \"Failed to fetch webpage.\"}), 500\n",
        "\n",
        "    # Optimized extraction: Get first few paragraphs to minimize processing\n",
        "    def extract_content(page_source):\n",
        "        soup = BeautifulSoup(page_source, 'html.parser')\n",
        "\n",
        "        # Limit extraction to first 5 <p> tags for speed\n",
        "        paragraphs = soup.find_all('p', limit=5)\n",
        "        paragraph_texts = [p.get_text() for p in paragraphs]\n",
        "\n",
        "        return paragraph_texts\n",
        "\n",
        "    # Extract relevant content from page\n",
        "    extracted_content = extract_content(page_source)\n",
        "\n",
        "    # Initialize OpenAI API\n",
        "    openai.api_key = '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'\n",
        "\n",
        "    # Function to determine relevance of content using OpenAI\n",
        "    def is_content_relevant(prompt, content):\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are an assistant that helps decide if content is relevant to a user's prompt.\"},\n",
        "                {\"role\": \"user\", \"content\": f\"Based on the instruction '{prompt}', is the following content relevant: {content}?\"}\n",
        "            ]\n",
        "        )\n",
        "        answer = response['choices'][0]['message']['content'].strip().lower()\n",
        "        return 'yes' in answer\n",
        "\n",
        "    # Filter content in larger chunks instead of individual paragraphs for speed\n",
        "    def filter_relevant_content(prompt, content_list):\n",
        "        chunked_content = \" \".join(content_list[:2])  # Create a chunk of the first two paragraphs\n",
        "        if is_content_relevant(prompt, chunked_content):\n",
        "            return chunked_content\n",
        "        return \"No relevant content found.\"\n",
        "\n",
        "    # Filter the content based on prompt\n",
        "    relevant_content = filter_relevant_content(prompt, extracted_content)\n",
        "\n",
        "    # Return JSON response\n",
        "    return jsonify({\n",
        "        \"url\": url,\n",
        "        \"prompt\": prompt,\n",
        "        \"relevant_content\": relevant_content\n",
        "    })\n",
        "\n",
        "# Run Flask in background thread\n",
        "def run_flask():\n",
        "    app.run(port=5000, debug=True, use_reloader=False)\n",
        "\n",
        "threading.Thread(target=run_flask).start()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-mo43yExmpd",
        "outputId": "d610a511-6a38-467d-c764-322819660adf"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Serving Flask app '__main__'\n",
            " * Debug mode: on\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "response = requests.post(\n",
        "    \"http://127.0.0.1:5000/scrape\",\n",
        "    json={\n",
        "        \"url\": \"https://en.wikipedia.org/wiki/Artificial_intelligence\",\n",
        "        \"prompt\": \"Find information about artificial intelligence\"\n",
        "    }\n",
        ")\n",
        "print(response.json())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tnWWbQFrq7Ao",
        "outputId": "5f60ac7f-6949-495d-ea8c-00f3e76fd140"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [23/Oct/2024 00:38:52] \"POST /scrape HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'prompt': 'Find information about artificial intelligence', 'relevant_content': '\\n Artificial intelligence (AI), in its broadest sense, is intelligence exhibited by machines, particularly computer systems. It is a field of research in computer science that develops and studies methods and software that enable machines to perceive their environment and use learning and intelligence to take actions that maximize their chances of achieving defined goals.[1] Such machines may be called AIs.\\n', 'url': 'https://en.wikipedia.org/wiki/Artificial_intelligence'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!lt --port 5000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SlGdzKDQp_-Q",
        "outputId": "d43dd97c-1bea-4db2-f16c-3295756a2f70"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "your url is: https://eleven-experts-decide.loca.lt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [23/Oct/2024 00:39:36] \"POST /scrape HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [23/Oct/2024 00:41:09] \"POST /scrape HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "^C\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
